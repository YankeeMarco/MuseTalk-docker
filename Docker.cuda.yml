# Use the NVIDIA CUDA image as a base
FROM nvidia/cuda:12.5.0-runtime-ubuntu22.04

# Install necessary packages including git, git-lfs, wget, and Python
RUN apt-get update && \
    apt-get install -y git git-lfs wget libgl1-mesa-glx  python3 python3-pip && \
    git lfs install && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set the working directory
WORKDIR /app

# Clone the https://github.com/TMElyralab/MuseTalk repository from Hugging Face
# ./models/
# ├── musetalk
# │   └── musetalk.json
# │   └── pytorch_model.bin
# ├── dwpose
# │   └── dw-ll_ucoco_384.pth
# ├── face-parse-bisent
# │   ├── 79999_iter.pth
# │   └── resnet18-5c106cde.pth
# ├── sd-vae-ft-mse
# │   ├── config.json
# │   └── diffusion_pytorch_model.bin
# └── whisper
#     └── tiny.pt
# Clone the MuseTalk repository from Hugging Face
RUN git clone https://huggingface.co/TMElyralab/MuseTalk.git /app/models/musetalk && \
    cd /app/models/musetalk && \
    git lfs pull

# Clone the DWPose repository from Hugging Face
RUN git clone https://huggingface.co/yzd-v/DWPose.git /app/models/dwpose && \
    cd /app/models/dwpose && \
    git lfs pull

# Clone the SD-VAE-FT-MSE repository from Hugging Face
RUN git clone https://huggingface.co/stabilityai/sd-vae-ft-mse.git /app/models/sd-vae-ft-mse && \
    cd /app/models/sd-vae-ft-mse && \
    git lfs pull

# Download the face-parse-bisent model and dependencies using wget
RUN mkdir -p /app/models/face-parse-bisent && \
    wget -O /app/models/face-parse-bisent/79999_iter.pth 'https://drive.usercontent.google.com/u/0/uc?id=154JgKpzCPW82qINcVieuPH3fZ2e0P812&export=download' && \
    wget -O /app/models/face-parse-bisent/resnet18-5c106cde.pth 'https://download.pytorch.org/models/resnet18-5c106cde.pth'

# Download the whisper model
RUN mkdir -p /app/models/whisper && \
    wget -O /app/models/whisper/tiny.pt 'https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt'

# Copy the requirements file into the container
COPY . /app

# Download and install PyTorch
RUN wget https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl && \
    pip install torch-2.0.1+cu118-cp310-cp310-linux_x86_64.whl

# Clean up to reduce image size (optional)
RUN rm torch-2.0.1+cu118-cp310-cp310-linux_x86_64.whl
# Install the required Python packages
RUN pip3 install --no-cache-dir -r requirements.txt

# Install specific mmlab packages
RUN pip3 install --no-cache-dir -U openmim && \
    mim install mmengine && \
    mim install "mmcv==2.0.1" && \
    mim install "mmdet==3.1.0" && \
    mim install "mmpose==1.1.0"

# Expose the necessary port 
EXPOSE 7866

# Command to run your application
CMD ["python3", "app.py"]

# To run the docker image, use docker-compose or run with volume mounted
# cd to the dir where the repo MuseTalk stays
# docker-compose build 
# docker-compose up